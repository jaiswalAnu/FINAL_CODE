{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwZUwWPObLL7llyOaMFd+p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxXGdEAoP9ib","executionInfo":{"status":"ok","timestamp":1685293435635,"user_tz":-330,"elapsed":152085,"user":{"displayName":"Anurag Jaiswal","userId":"15394690478600577889"}},"outputId":"420b2811-b8a9-4763-c3c1-263d51e8074a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","24/24 [==============================] - 12s 248ms/step - loss: 1.0412 - accuracy: 0.4453 - val_loss: 1.0117 - val_accuracy: 0.5189\n","Epoch 2/20\n","24/24 [==============================] - 6s 263ms/step - loss: 0.8693 - accuracy: 0.6209 - val_loss: 0.7937 - val_accuracy: 0.6622\n","Epoch 3/20\n","24/24 [==============================] - 5s 195ms/step - loss: 0.5819 - accuracy: 0.7500 - val_loss: 0.7833 - val_accuracy: 0.6730\n","Epoch 4/20\n","24/24 [==============================] - 6s 260ms/step - loss: 0.3827 - accuracy: 0.8541 - val_loss: 0.8595 - val_accuracy: 0.6811\n","Epoch 5/20\n","24/24 [==============================] - 5s 196ms/step - loss: 0.2261 - accuracy: 0.9189 - val_loss: 0.9463 - val_accuracy: 0.6568\n","Epoch 6/20\n","24/24 [==============================] - 5s 213ms/step - loss: 0.1504 - accuracy: 0.9500 - val_loss: 1.0420 - val_accuracy: 0.6459\n","Epoch 7/20\n","24/24 [==============================] - 6s 243ms/step - loss: 0.1053 - accuracy: 0.9689 - val_loss: 1.2088 - val_accuracy: 0.6324\n","Epoch 8/20\n","24/24 [==============================] - 5s 192ms/step - loss: 0.0847 - accuracy: 0.9723 - val_loss: 1.3143 - val_accuracy: 0.6432\n","Epoch 9/20\n","24/24 [==============================] - 6s 259ms/step - loss: 0.0682 - accuracy: 0.9777 - val_loss: 1.3287 - val_accuracy: 0.6486\n","Epoch 10/20\n","24/24 [==============================] - 5s 188ms/step - loss: 0.0513 - accuracy: 0.9804 - val_loss: 1.3789 - val_accuracy: 0.6595\n","Epoch 11/20\n","24/24 [==============================] - 5s 191ms/step - loss: 0.0344 - accuracy: 0.9892 - val_loss: 1.5387 - val_accuracy: 0.6432\n","Epoch 12/20\n","24/24 [==============================] - 6s 255ms/step - loss: 0.0284 - accuracy: 0.9912 - val_loss: 1.6306 - val_accuracy: 0.6649\n","Epoch 13/20\n","24/24 [==============================] - 5s 199ms/step - loss: 0.0261 - accuracy: 0.9912 - val_loss: 1.7179 - val_accuracy: 0.6541\n","Epoch 14/20\n","24/24 [==============================] - 6s 261ms/step - loss: 0.0265 - accuracy: 0.9912 - val_loss: 1.5761 - val_accuracy: 0.6622\n","Epoch 15/20\n","24/24 [==============================] - 5s 198ms/step - loss: 0.0635 - accuracy: 0.9811 - val_loss: 1.5428 - val_accuracy: 0.6486\n","Epoch 16/20\n","24/24 [==============================] - 5s 208ms/step - loss: 0.0423 - accuracy: 0.9878 - val_loss: 1.6108 - val_accuracy: 0.6649\n","Epoch 17/20\n","24/24 [==============================] - 6s 240ms/step - loss: 0.0289 - accuracy: 0.9905 - val_loss: 1.7152 - val_accuracy: 0.6595\n","Epoch 18/20\n","24/24 [==============================] - 5s 215ms/step - loss: 0.0220 - accuracy: 0.9946 - val_loss: 1.7312 - val_accuracy: 0.6595\n","Epoch 19/20\n","24/24 [==============================] - 7s 284ms/step - loss: 0.0449 - accuracy: 0.9885 - val_loss: 1.6761 - val_accuracy: 0.6514\n","Epoch 20/20\n","24/24 [==============================] - 5s 195ms/step - loss: 0.0365 - accuracy: 0.9892 - val_loss: 1.7581 - val_accuracy: 0.6324\n","15/15 [==============================] - 1s 25ms/step\n","Precision: 0.638080808080808\n","Recall: 0.6328868903381842\n","F1-Score: 0.6326461330185301\n","Accuracy: 0.6457883369330454\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, GRU, Dense, Input\n","from tensorflow.keras.models import Model\n","from keras.layers import concatenate\n","\n","# Load the dataset\n","df = pd.read_csv('/content/Laptop_datasets.csv')  # Replace 'dataset.csv' with your dataset file\n","\n","# Preprocessing the dataset\n","text_data = df['text'].values\n","aspect_data = df['aspect'].values\n","label_data = df['label'].values\n","\n","# Map labels to numerical values\n","label_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n","label_data = np.array([label_mapping[label] for label in label_data])\n","\n","# Split the dataset into training and testing sets\n","text_train, text_test, aspect_train, aspect_test, label_train, label_test = train_test_split(\n","    text_data, aspect_data, label_data, test_size=0.2, random_state=42\n",")\n","\n","# Tokenize the text data\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(text_train)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","text_train_seq = tokenizer.texts_to_sequences(text_train)\n","text_test_seq = tokenizer.texts_to_sequences(text_test)\n","\n","# Pad sequences for equal length\n","max_sequence_length = max(max(len(seq) for seq in text_train_seq), max(len(seq) for seq in text_test_seq))\n","text_train_seq = pad_sequences(text_train_seq, maxlen=max_sequence_length)\n","text_test_seq = pad_sequences(text_test_seq, maxlen=max_sequence_length)\n","\n","# Tokenize the aspect data\n","aspect_tokenizer = Tokenizer()\n","aspect_tokenizer.fit_on_texts(aspect_train)\n","aspect_vocab_size = len(aspect_tokenizer.word_index) + 1\n","\n","aspect_train_seq = aspect_tokenizer.texts_to_sequences(aspect_train)\n","aspect_test_seq = aspect_tokenizer.texts_to_sequences(aspect_test)\n","\n","# Pad sequences for equal length\n","max_aspect_length = max(max(len(seq) for seq in aspect_train_seq), max(len(seq) for seq in aspect_test_seq))\n","aspect_train_seq = pad_sequences(aspect_train_seq, maxlen=max_aspect_length)\n","aspect_test_seq = pad_sequences(aspect_test_seq, maxlen=max_aspect_length)\n","\n","# Create the model\n","embedding_dim = 100\n","hidden_units = 128\n","\n","text_input = Input(shape=(max_sequence_length,))\n","aspect_input = Input(shape=(max_aspect_length,))\n","\n","text_embedding = Embedding(vocab_size, embedding_dim)(text_input)\n","aspect_embedding = Embedding(aspect_vocab_size, embedding_dim)(aspect_input)\n","\n","text_gru = GRU(hidden_units)(text_embedding)\n","aspect_gru = GRU(hidden_units)(aspect_embedding)\n","\n","combined = concatenate([text_gru, aspect_gru])\n","output = Dense(3, activation='softmax')(combined)\n","\n","model = Model(inputs=[text_input, aspect_input], outputs=output)\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","batch_size = 64\n","epochs = 20\n","\n","model.fit([text_train_seq, aspect_train_seq], label_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n","\n","# Predict on the test set\n","predictions = model.predict([text_test_seq, aspect_test_seq])\n","predictions = np.argmax(predictions, axis=1)\n","\n","# Compute evaluation metrics\n","precision = precision_score(label_test, predictions, average='macro')\n","recall = recall_score(label_test, predictions, average='macro')\n","f1 = f1_score(label_test, predictions, average='macro')\n","accuracy = accuracy_score(label_test, predictions)\n","\n","print('Precision:', precision)\n","print('Recall:', recall)\n","print('F1-Score:', f1)\n","print('Accuracy:', accuracy)\n"]}]}